{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensorflow.keras.datasets import cifar100, cifar10, mnist\n",
    "\n",
    "from src.models.baseline.helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = Path(os.getcwd())\n",
    "root = cwd.parent.parent\n",
    "data = pathlib.PurePath(root, 'data')\n",
    "interim = pathlib.PurePath(data, 'interim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters & Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_param_grid ={\n",
    "    1 : {\n",
    "        'K' : [50, 100, 200, 500, 1000],\n",
    "        'epsilon' : 0.01\n",
    "    },\n",
    "    2 : {\n",
    "        'K' : 1000,\n",
    "        'epsilon' : [0.01, 0.05, 0.1, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "datasets = {\n",
    "    'MNIST' : {\n",
    "        'shape' : (28, 28, 1)\n",
    "        },\n",
    "    'CIFAR10' : {\n",
    "        'shape' : (32, 32, 3) \n",
    "    },\n",
    "    'CIFAR100' : {\n",
    "        'shape' : (32, 32, 3) \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_dir = interim\n",
    "seed = 8008"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, v in datasets.items():\n",
    "    x_train = v['data']['x_train']\n",
    "    x_test = v['data']['x_test']\n",
    "    y_train = v['data']['y_train']\n",
    "    y_test = v['data']['y_test']\n",
    "\n",
    "    for k in tqdm(test_param_grid[1]['K']):\n",
    "        print(\"Generating Partitions for {} dataset with {} clusters\".format(key, k))\n",
    "        x_vecs = flatten(x_train)\n",
    "        x, y = partition(x_vecs, k, SEED=seed, write_path=pathlib.PurePath(interim, key + str(k) + '_partitions.tsv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mu & Sigma\n",
    "\n",
    "Assumes Partitions Exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Partitions for MNIST dataset with 50 clusters\n",
      "Generating Mean & Variance for MNIST dataset with 50 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:28<01:54, 28.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Partitions for MNIST dataset with 100 clusters\n",
      "Generating Mean & Variance for MNIST dataset with 100 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:57<01:26, 28.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Partitions for MNIST dataset with 200 clusters\n",
      "Generating Mean & Variance for MNIST dataset with 200 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [01:24<00:56, 28.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Partitions for MNIST dataset with 500 clusters\n",
      "Generating Mean & Variance for MNIST dataset with 500 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [01:53<00:28, 28.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Partitions for MNIST dataset with 1000 clusters\n",
      "Generating Mean & Variance for MNIST dataset with 1000 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:26<00:00, 29.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Mean & Variance for MNIST dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Partitions for CIFAR10 dataset with 50 clusters\n",
      "Generating Mean & Variance for CIFAR10 dataset with 50 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [02:12<08:48, 132.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Partitions for CIFAR10 dataset with 100 clusters\n",
      "Generating Mean & Variance for CIFAR10 dataset with 100 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [04:53<07:28, 149.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Partitions for CIFAR10 dataset with 200 clusters\n",
      "Generating Mean & Variance for CIFAR10 dataset with 200 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [07:42<05:16, 158.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Partitions for CIFAR10 dataset with 500 clusters\n",
      "Generating Mean & Variance for CIFAR10 dataset with 500 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [10:20<02:38, 158.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Partitions for CIFAR10 dataset with 1000 clusters\n",
      "Generating Mean & Variance for CIFAR10 dataset with 1000 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [13:03<00:00, 156.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Mean & Variance for CIFAR10 dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Partitions for CIFAR100 dataset with 50 clusters\n",
      "Generating Mean & Variance for CIFAR100 dataset with 50 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [02:42<10:51, 162.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Partitions for CIFAR100 dataset with 100 clusters\n",
      "Generating Mean & Variance for CIFAR100 dataset with 100 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [06:00<09:09, 183.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Partitions for CIFAR100 dataset with 200 clusters\n",
      "Generating Mean & Variance for CIFAR100 dataset with 200 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [08:43<05:48, 174.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Partitions for CIFAR100 dataset with 500 clusters\n",
      "Generating Mean & Variance for CIFAR100 dataset with 500 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [11:12<02:44, 164.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Partitions for CIFAR100 dataset with 1000 clusters\n",
      "Generating Mean & Variance for CIFAR100 dataset with 1000 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [13:29<00:00, 161.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Mean & Variance for CIFAR100 dataset\n"
     ]
    }
   ],
   "source": [
    "for key, v in datasets.items():\n",
    "\n",
    "    storage = {}\n",
    "\n",
    "    for k in tqdm(test_param_grid[1]['K']):\n",
    "        dir = pathlib.PurePath(partition_dir, key + str(k) + '_partitions.tsv')\n",
    "        if Path(dir.as_posix()).exists():\n",
    "            print(\"Loading Partitions for {} dataset with {} clusters\".format(key, k))\n",
    "            with open(dir) as f:\n",
    "                lines = f.readlines()\n",
    "            lines = [line.rstrip() for line in lines]\n",
    "            x = []\n",
    "            y = []\n",
    "            for line in lines:\n",
    "                tokens = line.split()\n",
    "                x_vec = np.zeros(len(tokens)-1)\n",
    "                for i in range(len(tokens)-1):\n",
    "                    x_vec[i] = float(tokens[i])\n",
    "\n",
    "                x.append(x_vec)\n",
    "                y.append(int(tokens[-1]))\n",
    "\n",
    "        print(\"Generating Mean & Variance for {} dataset with {} clusters\".format(key, k))\n",
    "        members = groupClusters(x, y)\n",
    "        mu, sigma = computeMultivariateGaussianParameters(members)\n",
    "\n",
    "        storage[k] = (mu, sigma)\n",
    "\n",
    "        # Explicit Garbage Collection to Save Memory\n",
    "        \n",
    "        del x \n",
    "        del y\n",
    "        del members\n",
    "        del mu\n",
    "        del sigma\n",
    "\n",
    "    print(\"Saving Mean & Variance for {} dataset\".format(key))\n",
    "    with open(pathlib.PurePath(interim, \"meanvar{}.pkl\".format(key)), 'wb') as f:\n",
    "        pickle.dump(storage, f)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81fa8eaa475578715194bba22f072033b1fc3ca7ac2ce2b8cbb68c8b416ded27"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tfgpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
