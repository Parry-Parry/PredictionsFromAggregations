{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verification of Thesis Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.models.baseline.kmeans import runKmeans\n",
    "from tensorflow.keras.datasets import cifar100, cifar10, mnist\n",
    "\n",
    "from src.models.baseline.helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = Path(os.getcwd())\n",
    "root = cwd.parent.parent\n",
    "data = pathlib.PurePath(root, 'data')\n",
    "history = pathlib.PurePath(data, 'history')\n",
    "interim = pathlib.PurePath(data, 'interim')\n",
    "res = pathlib.PurePath(data, 'results', 'thesis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_param_grid ={\n",
    "    1 : {\n",
    "        'K' : [50, 100, 200, 500, 1000],\n",
    "        'epsilon' : 0.01\n",
    "    },\n",
    "    2 : {\n",
    "        'K' : 1000,\n",
    "        'epsilon' : [0.01, 0.05, 0.1, 1.0]\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "test_param_grid ={\n",
    "    1 : {\n",
    "        'K' : [1000],\n",
    "        'epsilon' : 0.01\n",
    "    },\n",
    "    2 : {\n",
    "        'K' : 1000,\n",
    "        'epsilon' : [0.01, 0.05, 0.1, 1.0]\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "model_param_grid = {\n",
    "    'MNIST' : {\n",
    "        'batch_size' : 1000,\n",
    "        'epochs' : 15,\n",
    "        'save_history' : True,\n",
    "        'path' : history\n",
    "    },\n",
    "    'CIFAR10' : {\n",
    "        'batch_size' : 64,\n",
    "        'epochs' : 30,\n",
    "        'save_history' : True,\n",
    "        'path' : history\n",
    "    },\n",
    "    'CIFAR100' : {\n",
    "        'batch_size' : 64,\n",
    "        'epochs' : 30,\n",
    "        'save_history' : True,\n",
    "        'path' : history\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    'MNIST' : {\n",
    "        'data' : dataset_normalize(mnist.load_data()),\n",
    "        'shape' : (28, 28, 1)\n",
    "        },\n",
    "    'CIFAR10' : {\n",
    "       'data' : dataset_normalize(cifar10.load_data()),\n",
    "       'shape' : (32, 32, 3) \n",
    "    },\n",
    "    'CIFAR100' : {\n",
    "        'data' : dataset_normalize(cifar100.load_data()),\n",
    "       'shape' : (32, 32, 3) \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets =  {\n",
    "    'CIFAR10' : {\n",
    "       'data' : dataset_normalize(cifar10.load_data()),\n",
    "       'shape' : (32, 32, 3) \n",
    "    },\n",
    "    'CIFAR100' : {\n",
    "        'data' : dataset_normalize(cifar100.load_data()),\n",
    "       'shape' : (32, 32, 3) \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_dir = interim\n",
    "seed = 8008"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 1 (Variable K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Test 1 on CIFAR10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Partitions for CIFAR10 dataset with 50 clusters\n",
      "y_train shape: (50000,)\n",
      "Accuracy on K-Means : 0.2674\n",
      "Accuracy on Epsilon_Neighbourhood : 0.2902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [20:17<1:21:09, 1217.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Complete_Information : 0.7836\n",
      "Loading Partitions for CIFAR10 dataset with 100 clusters\n",
      "y_train shape: (50000,)\n",
      "Accuracy on K-Means : 0.2822\n",
      "Accuracy on Epsilon_Neighbourhood : 0.3088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [41:41<1:02:50, 1256.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Complete_Information : 0.7743\n",
      "Loading Partitions for CIFAR10 dataset with 200 clusters\n",
      "y_train shape: (50000,)\n",
      "Accuracy on K-Means : 0.3084\n",
      "Accuracy on Epsilon_Neighbourhood : 0.3214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [1:03:04<42:17, 1268.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Complete_Information : 0.7861\n",
      "Loading Partitions for CIFAR10 dataset with 500 clusters\n",
      "y_train shape: (50000,)\n",
      "Accuracy on K-Means : 0.3302\n",
      "Accuracy on Epsilon_Neighbourhood : 0.2801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [1:24:39<21:19, 1279.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Complete_Information : 0.7752\n",
      "Loading Partitions for CIFAR10 dataset with 1000 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [1:30:04<22:31, 1351.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape: (50000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "attempt to get argmax of an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\SUMMER 2022\\PROJECT\\PredictionsFromAggregations\\notebooks\\baseline\\testing.ipynb Cell 12'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SUMMER%202022/PROJECT/PredictionsFromAggregations/notebooks/baseline/testing.ipynb#ch0000011?line=30'>31</a>\u001b[0m     x_vecs \u001b[39m=\u001b[39m flatten(x_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SUMMER%202022/PROJECT/PredictionsFromAggregations/notebooks/baseline/testing.ipynb#ch0000011?line=31'>32</a>\u001b[0m     x, y \u001b[39m=\u001b[39m partition(x_vecs, k, SEED\u001b[39m=\u001b[39mseed, write_path\u001b[39m=\u001b[39mpathlib\u001b[39m.\u001b[39mPurePath(interim, key \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(k) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_partitions.tsv\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/SUMMER%202022/PROJECT/PredictionsFromAggregations/notebooks/baseline/testing.ipynb#ch0000011?line=33'>34</a>\u001b[0m kmeans \u001b[39m=\u001b[39m runKmeans(k, (x_train, x_test), (y_train, y_test), v[\u001b[39m'\u001b[39;49m\u001b[39mshape\u001b[39;49m\u001b[39m'\u001b[39;49m], key, model_param_grid[key])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SUMMER%202022/PROJECT/PredictionsFromAggregations/notebooks/baseline/testing.ipynb#ch0000011?line=34'>35</a>\u001b[0m results \u001b[39m=\u001b[39m runTest(k, test_param_grid[\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mepsilon\u001b[39m\u001b[39m'\u001b[39m], (x_train, x_test), (y_train, y_test), (x, y), v[\u001b[39m'\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m'\u001b[39m], model_param_grid[key], key, (\u001b[39mFalse\u001b[39;00m, \u001b[39mTrue\u001b[39;00m, \u001b[39mTrue\u001b[39;00m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SUMMER%202022/PROJECT/PredictionsFromAggregations/notebooks/baseline/testing.ipynb#ch0000011?line=36'>37</a>\u001b[0m sets \u001b[39m=\u001b[39m [kmeans, results[\u001b[39m'\u001b[39m\u001b[39mepsilon\u001b[39m\u001b[39m'\u001b[39m], results[\u001b[39m'\u001b[39m\u001b[39mcomplete\u001b[39m\u001b[39m'\u001b[39m]]\n",
      "File \u001b[1;32md:\\CONDA\\envs\\tfgpu\\lib\\site-packages\\src-0.1.0-py3.9.egg\\src\\models\\baseline\\kmeans.py:56\u001b[0m, in \u001b[0;36mrunKmeans\u001b[1;34m(K, X, Y, shape, dataset_name, params)\u001b[0m\n\u001b[0;32m     52\u001b[0m kmeans\u001b[39m.\u001b[39mfit(x_train)\n\u001b[0;32m     54\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39my_train shape: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(y_train\u001b[39m.\u001b[39mshape))\n\u001b[1;32m---> 56\u001b[0m cluster_labels \u001b[39m=\u001b[39m infer_cluster_labels(kmeans, y_train)\n\u001b[0;32m     58\u001b[0m labels \u001b[39m=\u001b[39m []\n\u001b[0;32m     60\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(K):\n",
      "File \u001b[1;32md:\\CONDA\\envs\\tfgpu\\lib\\site-packages\\src-0.1.0-py3.9.egg\\src\\models\\baseline\\kmeans.py:22\u001b[0m, in \u001b[0;36minfer_cluster_labels\u001b[1;34m(kmeans, actual_labels)\u001b[0m\n\u001b[0;32m     19\u001b[0m     counts \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mbincount(np\u001b[39m.\u001b[39msqueeze(labels))\n\u001b[0;32m     21\u001b[0m \u001b[39m# assign the cluster to a value in the inferred_labels dictionary\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39;49margmax(counts) \u001b[39min\u001b[39;00m inferred_labels:\n\u001b[0;32m     23\u001b[0m     \u001b[39m# append the new number to the existing array at this slot\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     inferred_labels[np\u001b[39m.\u001b[39margmax(counts)]\u001b[39m.\u001b[39mappend(i)\n\u001b[0;32m     25\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m     \u001b[39m# create a new array in this slot\u001b[39;00m\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32md:\\CONDA\\envs\\tfgpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1195\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(a, axis, out)\u001b[0m\n\u001b[0;32m   1121\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_argmax_dispatcher)\n\u001b[0;32m   1122\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39margmax\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1123\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m \u001b[39m    Returns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1193\u001b[0m \n\u001b[0;32m   1194\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1195\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39margmax\u001b[39;49m\u001b[39m'\u001b[39;49m, axis\u001b[39m=\u001b[39;49maxis, out\u001b[39m=\u001b[39;49mout)\n",
      "File \u001b[1;32md:\\CONDA\\envs\\tfgpu\\lib\\site-packages\\numpy\\core\\fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[39m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[39m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[39m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[39m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "\u001b[1;31mValueError\u001b[0m: attempt to get argmax of an empty sequence"
     ]
    }
   ],
   "source": [
    "for key, v in datasets.items():\n",
    "    \n",
    "    intermediate = []\n",
    "\n",
    "    x_train = v['data']['x_train']\n",
    "    x_test = v['data']['x_test']\n",
    "    y_train = v['data']['y_train']\n",
    "    y_test = v['data']['y_test']\n",
    "\n",
    "    print(\"Running Test 1 on {}...\".format(key))\n",
    "\n",
    "    for k in tqdm(test_param_grid[1]['K']):\n",
    "        dir = pathlib.PurePath(partition_dir, key + str(k) + '_partitions.tsv')\n",
    "        if Path(dir.as_posix()).exists():\n",
    "            print(\"Loading Partitions for {} dataset with {} clusters\".format(key, k))\n",
    "            with open(dir) as f:\n",
    "                lines = f.readlines()\n",
    "            lines = [line.rstrip() for line in lines]\n",
    "            x = []\n",
    "            y = []\n",
    "            for line in lines:\n",
    "                tokens = line.split()\n",
    "                x_vec = np.zeros(len(tokens)-1)\n",
    "                for i in range(len(tokens)-1):\n",
    "                    x_vec[i] = float(tokens[i])\n",
    "\n",
    "                x.append(x_vec)\n",
    "                y.append(int(tokens[-1]))\n",
    "        else:\n",
    "            print(\"Generating Partitions for {} dataset with {} clusters\".format(key, k))\n",
    "            x_vecs = flatten(x_train)\n",
    "            x, y = partition(x_vecs, k, SEED=seed, write_path=pathlib.PurePath(interim, key + str(k) + '_partitions.tsv'))\n",
    "\n",
    "        kmeans = runKmeans(k, (x_train, x_test), (y_train, y_test), v['shape'], key, model_param_grid[key])\n",
    "        results = runTest(k, test_param_grid[1]['epsilon'], (x_train, x_test), (y_train, y_test), (x, y), v['shape'], model_param_grid[key], key, (False, True, True))\n",
    "        \n",
    "        sets = [kmeans, results['epsilon'], results['complete']]\n",
    "        for set in sets:\n",
    "            set['K'] = k\n",
    "            intermediate.append(set)\n",
    "\n",
    "    metrics = pd.DataFrame(intermediate)\n",
    "    metrics.to_csv(pathlib.PurePath(res, 'strategy1_{}.csv'.format(key)))\n",
    "    print(\"Test 1 Completed Successfully for {}\".format(key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 2 (Variable $\\epsilon$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Partitions for MNIST dataset with 1000 clusters\n",
      "Running Test 2 on MNIST...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [03:57<11:51, 237.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Epsilon_Neighbourhood : 0.9673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [07:57<07:57, 238.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Epsilon_Neighbourhood : 0.9671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [11:53<03:57, 237.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Epsilon_Neighbourhood : 0.9631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [15:50<00:00, 237.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Epsilon_Neighbourhood : 0.9569\n",
      "Test 2 Completed Successfully for MNIST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for key, v in datasets.items():\n",
    "\n",
    "    intermediate = []\n",
    "\n",
    "    x_train = v['data']['x_train']\n",
    "    x_test = v['data']['x_test']\n",
    "    y_train = v['data']['y_train']\n",
    "    y_test = v['data']['y_test']\n",
    "\n",
    "    k = test_param_grid[2]['K']\n",
    "    dir = pathlib.PurePath(partition_dir, key + str(k) + '_partitions.tsv')\n",
    "    \n",
    "    if Path(dir.as_posix()).exists():\n",
    "        print(\"Loading Partitions for {} dataset with {} clusters\".format(key, k))\n",
    "        with open(dir) as f:\n",
    "            lines = f.readlines()\n",
    "        lines = [line.rstrip() for line in lines]\n",
    "        x = []\n",
    "        y = []\n",
    "        for line in lines:\n",
    "            tokens = line.split()\n",
    "            x_vec = np.zeros(len(tokens)-1)\n",
    "            for i in range(len(tokens)-1):\n",
    "                x_vec[i] = float(tokens[i])\n",
    "\n",
    "            x.append(x_vec)\n",
    "            y.append(int(tokens[-1]))\n",
    "    else: \n",
    "        print(\"Generating Partitions for {} dataset with {} clusters\".format(key, k))\n",
    "        x_vecs = flatten(x_train)\n",
    "        x, y = partition(x_vecs, k, SEED=seed, write_path=pathlib.PurePath(interim, key + str(k) + '_partitions.tsv'))\n",
    "\n",
    "    print(\"Running Test 2 on {}...\".format(key))\n",
    "\n",
    "    for e in tqdm(test_param_grid[2]['epsilon']): \n",
    "        results = runTest(test_param_grid[2]['K'], e, (x_train, x_test), (y_train, y_test), (x, y), v['shape'], model_param_grid[key], key, (False, True, False))\n",
    "        results = results['epsilon']\n",
    "        results['dataset'] = key\n",
    "        results['Epsilon'] = e\n",
    "\n",
    "        intermediate.append(results)\n",
    "    \n",
    "    metrics = pd.DataFrame(intermediate)\n",
    "    metrics.to_csv(pathlib.PurePath(res, 'strategy2_{}.csv'.format(key)))\n",
    "    print(\"Test 2 Completed Successfully for {}\".format(key))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81fa8eaa475578715194bba22f072033b1fc3ca7ac2ce2b8cbb68c8b416ded27"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tfgpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
