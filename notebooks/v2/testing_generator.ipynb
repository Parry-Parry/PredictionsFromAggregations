{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "from pathlib import Path, PurePath\n",
    "from collections import defaultdict\n",
    "\n",
    "from src.models.structures import *\n",
    "from src.models.intermediate_robust_generator.model import *\n",
    "from src.models.lstm_based.helper import retrieve_dataset, aggregate\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import sklearn as sk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER = 2048\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 1000\n",
    "EPSILON = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "SEED = 8008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARTITION_DIR = \"D:\\SUMMER_2022\\PROJECT\\PredictionsFromAggregations\\data\\interim\\lstm\"\n",
    "DATASET = 'MNIST'\n",
    "DIR = \"D:\\SUMMER_2022\\PROJECT\\PredictionsFromAggregations\\models\\v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name, data = retrieve_dataset(DATASET)\n",
    "dataset = Dataset(name, data)\n",
    "mean, dataset = aggregate(dataset, K, PARTITION_DIR, SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a, b, c, d = dataset.x_train.shape\n",
    "n_classes = len(np.unique(dataset.y_train))\n",
    "\n",
    "x_train = dataset.x_train.reshape(a, b*c*d)\n",
    "y_train = tfk.utils.to_categorical(dataset.y_train, n_classes)\n",
    "x_test = dataset.x_test.reshape(dataset.x_test.shape[0], b*c*d)\n",
    "y_test = tfk.utils.to_categorical(dataset.y_test, n_classes)\n",
    "\n",
    "x_train, x_val, y_train, y_val = sk.model_selection.train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "\"\"\"I present the current worst function in the codebase\"\"\"\n",
    "tf_convert = lambda x, y, type : (tf.data.Dataset.from_tensor_slices((tf.cast(x, type), tf.cast(y, type)))).shuffle(BUFFER).batch(BATCH_SIZE, drop_remainder=True).cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_set = tf_convert(x_train, y_train, tf.uint8)\n",
    "test_set = tf_convert(x_test, y_test, tf.uint8)\n",
    "val_set = tf_convert(x_val, y_val, tf.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merger = tfk.layers.BiDirectional(tfk.layers.LSTM(mean, activation='relu', name='merging_layer'))\n",
    "\n",
    "config = generator_config(b*c*d, 10, n_classes, 4, None, merger)\n",
    "model = stochastic_model(config)\n",
    "\n",
    "step = tf.Variable(0, trainable=False)\n",
    "schedule = tf.optimizers.schedules.PiecewiseConstantDecay(\n",
    "[10000, 15000], [1e-0, 1e-1, 1e-2])\n",
    "lr = 1e-1 * schedule(step)\n",
    "wd = lambda: 1e-4 * schedule(step)\n",
    "\n",
    "optim = tfa.optimizers.AdamW(learning_rate=lr, weight_decay=wd)\n",
    "loss_fn = generator_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = Result(defaultdict(list), {}, defaultdict(list))\n",
    "\n",
    "train_acc_metric = tfk.metrics.SparseCategoricalAccuracy()\n",
    "val_acc_metric = tfk.metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(args.epochs):\n",
    "    logger.info('Epoch {}...'.format(epoch))\n",
    "    for step, (x_batch, y_batch) in enumerate(train_set): \n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = model(x_batch)\n",
    "            loss_value = loss_fn(y_batch, pred, [gen.dense.kernel for gen in model.generators])\n",
    "        results.history[epoch].append(loss_value)\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optim.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        train_acc_metric.update_state(y_batch, pred)\n",
    "\n",
    "    train_acc = train_acc_metric.result()\n",
    "    results.acc_score[epoch].append(train_acc)\n",
    "    logger.info(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "\n",
    "    train_acc_metric.reset_states()\n",
    "\n",
    "    if step % BATCH_SIZE == 0:\n",
    "        logger.info(\n",
    "            \"Training loss (for one batch) at step %d: %.4f\"\n",
    "            % (step, float(loss_value))\n",
    "        )\n",
    "\n",
    "    for x_batch, y_batch in val_set:\n",
    "        val_pred = model(x_batch, training=False)\n",
    "        val_acc_metric.update_state(y_batch, val_pred)\n",
    "    val_acc = val_acc_metric.result()\n",
    "    results.val_acc_score[epoch] = val_acc\n",
    "    val_acc_metric.reset_states()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
